[{"path":"index.html","id":"welcome","chapter":"Welcome!","heading":"Welcome!","text":"website documents gapclosing package R. package helps answers questions form: degree hypothetical intervention close gaps across populations?causal question paramount importance want intervene reduce disparities across categories gender, race, class. answer question subject companion papaer package.Lundberg, Ian. 2022. gap-closing estimand: causal approach study interventions close disparities across social categories. Sociological Methods Research.get started methods, first install R RStudio. install package CRAN.can now lots things!Estimate treatment outcome prediction functions statistical machine learning methodsCombine doubly-robust estimators gap-closing estimandsProduce confidence intervals bootstrapVisualize resultQuestions comments Ian Lundberg, ilundberg@cornell.edu.","code":"\ninstall.packages(\"gapclosing\")"},{"path":"big-idea.html","id":"big-idea","chapter":"1 Big idea","heading":"1 Big idea","text":"part introduces big ideas: , data look like, ’s going hood. closes basic package functionality.","code":""},{"path":"big-idea.html","id":"motivation","chapter":"1 Big idea","heading":"1.1 Motivation","text":"Gaps across social categories like race, class, gender important understand. like know whether anything can close gaps. intervened reduce incarceration increase access education? interventions close gaps across categories race, class, gender?types questions core growing literature epidemiology addresses questions techniques causal decomposition analysis (VanderWeele Robinson (2014), Jackson VanderWeele (2018), Jackson (2020)). package provides software support inquiry gap-closing estimands, discussed Lundberg (2022).guiding principle distinguish human tasks software tasks can automated.human, :define interventionmake causal assumptions identificationspecify treatment model /outcome modelThen package :estimate modelsproduce doubly-robust estimatessample split improve convergenceestimate standard errors bootstrappingvisualize result","code":""},{"path":"big-idea.html","id":"data-structure","chapter":"1 Big idea","heading":"1.2 Data structure","text":"data frame data, gap-defining category race, gender, class. binary treatment variable counterfactually different individual. want know degree intervention change treatment close gaps across categories.boxes present example.Example. Suppose following data.\\(X\\) (category): Category interest, taking values {, B, C}\\(T\\) (treatment): Binary treatment variable, taking values 0 1\\(L\\) (confounder): continuous confounding variable, Uniform(-1,1)\\(Y\\) (outcome): continuous outcome variable, conditionally normal","code":"\nset.seed(08544)\nlibrary(gapclosing)\nlibrary(dplyr)\nlibrary(ggplot2)\nsimulated_data <- generate_simulated_data(n = 1000)\nhead(simulated_data)\n#>   category confounder treatment    outcome\n#> 1        C   1.640509         1 -0.7970377\n#> 2        B   1.032373         0  1.9304909\n#> 3        C   2.217630         1 -0.2009803\n#> 4        A  -1.642914         0 -0.5015702\n#> 5        A  -1.844897         0 -1.2027025\n#> 6        A  -2.415100         0 -3.9288188"},{"path":"big-idea.html","id":"coding-from-scratch","chapter":"1 Big idea","heading":"1.3 Coding from scratch","text":"simple models, can carry gap-closing analysis without software package. First, fit prediction function outcome function category interest, confounders, treatment.everyone sample, predict counterfactual treatment value (e.g., treatment = 1).Average counterfactual estimates within category.software package supports steps well complex things might want:three estimation strategies\noutcome prediction\ntreatment prediction\ndoubly robust estimation\noutcome predictiontreatment predictiondoubly robust estimationmachine learning prediction functionscounterfactual treatments differ across unitsbootstrapping standard errorseasy visualization","code":"\nexample_ols <- lm(outcome ~ category*treatment + confounder,\n                  data = simulated_data)\nfitted <- simulated_data %>%\n  mutate(outcome_under_treatment_1 = predict(example_ols,\n                                             newdata = simulated_data %>%\n                                               mutate(treatment = 1)))\nfitted %>%\n  # Group by the category of interest\n  group_by(category) %>%\n  # Take the average prediction\n  summarize(factual = mean(outcome),\n            counterfactual = mean(outcome_under_treatment_1))\n#> # A tibble: 3 × 3\n#>   category factual counterfactual\n#>   <chr>      <dbl>          <dbl>\n#> 1 A        -1.09           0.0488\n#> 2 B        -0.0384        -0.0370\n#> 3 C         0.495          0.132"},{"path":"big-idea.html","id":"basic-package-functionality","chapter":"1 Big idea","heading":"1.4 Basic package functionality","text":"gapclosing() function estimates gaps across categories degree close specified counterfactual_assignments treatment.default, function following:Fit logistic regression predict treatment assignmentFit OLS regression predict outcomesCombine two doubly-robust estimator estimated single sampleReturn gapclosing object supports summary, print, plot functions.example, plot(estimate) function produces following visualization. factual outcomes unequal across categories, counterfactual outcomes roughly equal. simulated setting, intervention almost entirely closes gaps across categories.disparityplot() function lets us zoom factual counterfactual disparity two categories, interest. case, see intervention lifts outcomes category comparable category B. disparityplot ggplot2 object can customized passing additional layers.summary function print estimates, standard errors, confidence intervals results.","code":"\nestimate <- gapclosing(\n  data = simulated_data,\n  counterfactual_assignments = 1,\n  outcome_formula = formula(outcome ~ confounder + category*treatment),\n  treatment_formula = formula(treatment ~ confounder + category),\n  category_name = \"category\",\n  se = TRUE,\n  # Setting bootstrap_samples very low to speed this tutorial\n  # Should be set higher in practice\n  bootstrap_samples = 20,\n  # You can process the bootstrap in parallel with as many cores as available\n  parallel_cores = 1\n)\nplot(estimate)\ndisparityplot(estimate, category_A = \"A\", category_B = \"B\") +\n  ggtitle(\"A disparityplot()\")\nsummary(estimate)\n#> Gap-closing estimates using doubly_robust estimation on one sample.\n#> \n#> Treatment model was glm estimation with model formula:\n#> formula(treatment ~ confounder + category)\n#> \n#> Outcome model was lm estimation with model formula:\n#> formula(outcome ~ confounder + category * treatment)\n#> \n#> Factual estimates are means within and disparities across category.\n#> Counterfactual estimates are under an intervention to set  to 1.\n#> Standard errors are calculated from 20 bootstrap samples.\n#> \n#> Factual mean outcomes:\n#> # A tibble: 3 × 5\n#>   category estimate     se ci.min  ci.max\n#>   <chr>       <dbl>  <dbl>  <dbl>   <dbl>\n#> 1 A         -1.09   0.0863 -1.26  -0.921 \n#> 2 B         -0.0384 0.0675 -0.171  0.0938\n#> 3 C          0.495  0.0821  0.334  0.656 \n#> \n#> Counterfactual mean outcomes (post-intervention means):\n#> # A tibble: 3 × 5\n#>   category estimate     se  ci.min ci.max\n#>   <chr>       <dbl>  <dbl>   <dbl>  <dbl>\n#> 1 A         -0.102  0.152  -0.400   0.196\n#> 2 B          0.0409 0.0866 -0.129   0.211\n#> 3 C          0.0805 0.0893 -0.0945  0.256\n#> \n#> Factual disparities:\n#> # A tibble: 6 × 5\n#>   category estimate    se ci.min ci.max\n#>   <chr>       <dbl> <dbl>  <dbl>  <dbl>\n#> 1 A - B      -1.05  0.104 -1.26  -0.849\n#> 2 A - C      -1.59  0.120 -1.82  -1.35 \n#> 3 B - A       1.05  0.104  0.849  1.26 \n#> 4 B - C      -0.534 0.109 -0.747 -0.321\n#> 5 C - A       1.59  0.120  1.35   1.82 \n#> 6 C - B       0.534 0.109  0.321  0.747\n#> \n#> Counterfactual disparities (gap-closing estimands):\n#> # A tibble: 6 × 5\n#>   category estimate    se ci.min ci.max\n#>   <chr>       <dbl> <dbl>  <dbl>  <dbl>\n#> 1 A - B     -0.143  0.149 -0.435  0.150\n#> 2 A - C     -0.183  0.178 -0.532  0.167\n#> 3 B - A      0.143  0.149 -0.150  0.435\n#> 4 B - C     -0.0396 0.110 -0.255  0.175\n#> 5 C - A      0.183  0.178 -0.167  0.532\n#> 6 C - B      0.0396 0.110 -0.175  0.255\n#> \n#> Additive gap closed: Counterfactual - Factual\n#> # A tibble: 6 × 5\n#>   category estimate     se ci.min ci.max\n#>   <chr>       <dbl>  <dbl>  <dbl>  <dbl>\n#> 1 A - B      -0.909 0.120  -1.15  -0.673\n#> 2 A - C      -1.40  0.112  -1.62  -1.18 \n#> 3 B - A       0.909 0.120   0.673  1.15 \n#> 4 B - C      -0.494 0.0644 -0.620 -0.368\n#> 5 C - A       1.40  0.112   1.18   1.62 \n#> 6 C - B       0.494 0.0644  0.368  0.620\n#> \n#> Proportional gap closed: (Counterfactual - Factual) / Factual\n#> # A tibble: 6 × 5\n#>   category estimate    se ci.min ci.max\n#>   <chr>       <dbl> <dbl>  <dbl>  <dbl>\n#> 1 A - B       0.864 0.141  0.588   1.14\n#> 2 A - C       0.885 0.105  0.679   1.09\n#> 3 B - A       0.864 0.141  0.588   1.14\n#> 4 B - C       0.926 0.207  0.520   1.33\n#> 5 C - A       0.885 0.105  0.679   1.09\n#> 6 C - B       0.926 0.207  0.520   1.33\n#> \n#> Type plot(name_of_this_object) to visualize results."},{"path":"concepts-in-detail.html","id":"concepts-in-detail","chapter":"2 Concepts in detail","heading":"2 Concepts in detail","text":"section provides detailed overview use gapclosing().structured perspective three key tasks researcher:define interventionmake causal assumptions identificationspecify treatment /outcome modelAlong way, section introduces many possible arguments gapclosing() call.","code":""},{"path":"concepts-in-detail.html","id":"define-the-intervention","chapter":"2 Concepts in detail","heading":"2.1 Define the intervention","text":"answer gap-closing question, first need define intervention . treatment value units counterfactually assigned? several options.Option 1. Set counterfactual_assignments = 0 counterfactual_assignments = 1. case, studying disparity expect random person category assigned control assigned treatment (respectively).Option 2. Set counterfactual_assignments = \\(\\pi\\) \\(\\pi\\) 0 1. case, studying disparity expect random person category assigned treatment probability \\(\\pi\\) control probability \\(1 - \\pi\\).Option 3. Assign treatments probability \\(\\pi_i\\) may differ person \\(\\). case, create vector \\(\\vec\\pi\\) length \\(n\\) pass counterfactual_assignments argument.Example. interested disparity across populations defined category persist counterfactual assignment set treatment value 1.\ncounterfactual_assignments = 1","code":""},{"path":"concepts-in-detail.html","id":"make-causal-assumptions-for-identification","chapter":"2 Concepts in detail","heading":"2.2 Make causal assumptions for identification","text":"package help step. Gap-closing estimands involve unobserved potential outcomes. unobserved, data tell us variables needed estimation. Instead, conceptual choice carried tools like Directed Acyclic Graphs (DAGs). See accompanying Lundberg (2022) paper identification.Example. Assume set variables \\(\\{X,L\\}\\) sufficient conditioning set identify gap-closing estimand. Formally, requires us assume within stratum \\(X\\) \\(L\\) expected value potential outcome \\(Y(1)\\) expected value among units factually \\(T = 1\\) within strata.\n\\[\\mathbb{E}(Y(1)\\mid X, L) = \\mathbb{E}(Y\\mid X, L, T = 1)\\]\nDAGs good way reason assumption: example, conditioning (depicted boxes) category confounder sufficient identify causal effect treatment (blue edge DAG), blocks backdoor paths treatment outcome. Notably, gap-closing estimand makes claims causal effect category since counterfactual defined treatment .select sufficient conditioning set, predictors appear treatment /outcome model used estimation.","code":""},{"path":"concepts-in-detail.html","id":"specify-a-treatment-model-andor-an-outcome-model","chapter":"2 Concepts in detail","heading":"2.3 Specify a treatment model and/or an outcome model","text":"need estimate one (1) probability treatment given confounders (2) conditional mean outcome given treatment confounders. providing one following.Provide treatment_formula binary treatment variable left side confounders right side. formula used treatment_algorithm (see next step) estimate probability treatment given confounders unit. , sample-average outcome counterfactual assignment given treatment can estimated inverse probability weighting among units factually received treatment.Example.treatment_formula = formula(outcome ~ confounder + category)Provide outcome_formula outcome variable left side treatment confounders right side. formula used outcome_algorithm (see next step) estimate conditional mean outcome given treatment confounders. , can used predict potential outcome unit expect treatment interest, given confounding variables.Example.outcome_formula = formula(outcome ~ confounder + category*treatment)Whether treatment_formula outcome_formula left NULL determine estimation procedure.treatment_formula provided, estimation inverse probability treatment weighting.outcome_formula provided, estimation prediction unobserved potential outcomes (\\(g\\)-formula, see Hernán Robins (2021)).treatment_formula outcome_formula provided, primary estimate doubly robust, separate treatment outcome modeling estimates accessible returned object.","code":""},{"path":"concepts-in-detail.html","id":"choose-an-estimation-algorithm","chapter":"2 Concepts in detail","heading":"2.3.1 Choose an estimation algorithm","text":"treatment_formula outcome_formula handed treatment_algorithm outcome_algorithm, can take following values.glm (treatment_algorithm ): logistic regression model estimated glm function. important interactions among treatment, category, covariates, included explicitly.lm (outcome_algorithm ): OLS regression model estimated lm function. important interactions among treatment, category, covariates, included explicitly.ridge: ridge (.e. L2-penalized) logistic regression model (treatment_algorithm) linear regression model (outcome_algorithm) estimated glmnet function glmnet package (Friedman, Hastie, Tibshirani 2010), alpha = 0 indicate ridge penalty. Elastic net lasso regression supported approaches regularize coefficients exactly zero, chosen needed confounders (step 2) want coefficients regularized zero. penalty term chosen cv.glmnet set value lambda.min (see glmnet documentation). important interactions among treatment, category, covariates, included explicitly.gam: Generalized Additive Model (logistic used treatment_algorithm, linear used outcome_algorithm) estimated gam function mgcv package (Wood 2017). model formula specified mgcv documentation may include smooth terms s() continuous covariates. important interactions among treatment, category, covariates, included explicitly.ranger: random forest estimated ranger function ranger package (Wright Ziegler 2017). used treatment_algorithm, one forest fit predicted treatment probabilities truncated [.001,.999] range avoid extreme inverse probability treatment weights. used outcome_algorithm, forest estimated separately treated control units; treatment variable need included outcome_formula case.Example.treatment_algorithm = \"glm\"outcome_algorithm = \"lm\"data sample population selected unequal probabilities, can also use weight_name option pass estimation functions name sampling weight (variable data proportional inverse probability sample inclusion). omitted, simple random sample assumed.","code":""},{"path":"concepts-in-detail.html","id":"why-doubly-robust-a-side-note","chapter":"2 Concepts in detail","heading":"2.3.2 Why doubly robust? A side note","text":"Doubly-robust estimation yields advantages can conceptualized two ways.perspective parametric models, either treatment outcome model correct, estimator consistent (Bang Robins 2005). See Glynn Quinn (2010) accessible introduction.perspective machine learning, double robustness adjusts fact outcome modeling alone optimized wrong prediction task. outcome model optimized predict observed data, actual task predict predictor distribution different observed (treatment changed). opportunity improve predictions. building model treatment, can reweight residuals outcome model estimate average prediction error space want make predictions. Subtracting bias can improve outcome modeling estimator. pivot core moves toward targeted learning (Van der Laan Rose 2011) double machine learning (Chernozhukov et al. 2018), building long line research efficient estimation (Robins Rotnitzky 1995; Hahn 1998).Although double robustness strong mathematical properties, given application finite sample possible treatment outcome modeling outperform doubly-robust estimation. Therefore, package supports three approaches.","code":""},{"path":"concepts-in-detail.html","id":"why-sample-splitting-another-side-note","chapter":"2 Concepts in detail","heading":"2.3.3 Why sample splitting? Another side note","text":"Taking bias-correction view double robustness , clear sample splitting affords opportunity improvement: learn outcome model estimate average bias sample, might get poor estimate bias. reason, one consider using one sample (call data_learn) learn prediction functions another sample (call data_estimate) estimate bias aggregate estimate estimand.particular, option sample_split = \"cross_fit\" allows user specify estimation proceed cross-fitting procedure analogous cross-validation.Split sample folds \\(f = 1,\\dots,\\)n_folds (default n_folds = 2)Use folds except \\(f'\\) estimate treatment outcome modelsAggregate estimate using predictions \\(f'\\)Average estimate results (2) (3) repeated n_folds times fold playing role \\(f'\\) turnThis procedure Chernozhukov et al. (2018) argue critical double machine learning causal estimation, although type sample splitting new (Bickel 1982).sample_split = \"cross_fit\", default conduct 2-fold cross-fitting, can changed n_folds argument. user can also specify vector folds fold assignments length nrow(data), something particular setting make manual fold assignment preferable.Example. (default can left implicit)sample_split = \"one_sample\"","code":""},{"path":"concepts-in-detail.html","id":"produce-standard-errors","chapter":"2 Concepts in detail","heading":"2.3.4 Produce standard errors","text":"package supports bootstrapped standard error estimation. procedure bootstrap_method = \"simple\" (default) valid data simple random sample target population. case, bootstrap iteration conducts estimation resampled dataset selected replacement equal probabilities. standard error calculated standard deviation estimate across bootstrap samples, confidence intervals calculated normal approximation.Example. (line 2 default can left implicit)se = T\nbootstrap_samples = 1000In settings, sample size may small categories treatments interest may rare. cases, possible one simple bootstrap samples contain zero cases (treatment \\(\\times\\) category) cell interest. avoid problem, bootstrap_method = \"stratified\" conducts bootstrap resampling within blocks defined (treatment \\(\\times\\) category). procedure valid assume data selected random population within strata, across repeated samples true population proportion stratum remain .Many samples simple random samples. complex sample settings, users implement standard error procedures accurately capture sampling variation related data collected. way data collected motivate resampling strategy mimic sources variation sampling process, user can implement manually calling gapclosing calculate point estimate resampled dataset se = FALSE.","code":""},{"path":"machine-learning-examples.html","id":"machine-learning-examples","chapter":"3 Machine learning examples","heading":"3 Machine learning examples","text":"Suppose want relax parametric functional form assumptions plugging machine learning estimator. user, simply involves changing arguments gapclosing() function.","code":""},{"path":"machine-learning-examples.html","id":"generalized-additive-models","chapter":"3 Machine learning examples","heading":"3.1 Generalized Additive Models","text":"Perhaps concerned linearity assumptions: continuous confounder, instance, might actually nonlinear association outcome. know truth linear simulated example, practice never know. can address concern estimating GAM, using s() operator mgcv smooth terms (see Wood (2017)).","code":"\nestimate_gam <- gapclosing(\n  data = simulated_data,\n  counterfactual_assignments = 1,\n  outcome_formula = formula(outcome ~ s(confounder) + category*treatment),\n  treatment_formula = formula(treatment ~ s(confounder) + category),\n  category_name = \"category\",\n  treatment_algorithm = \"gam\",\n  outcome_algorithm = \"gam\",\n  sample_split = \"cross_fit\"\n  # Note: Standard errors with `se = TRUE` are supported.\n  # They are omitted here only to speed vignette build time.\n)"},{"path":"machine-learning-examples.html","id":"random-forests","chapter":"3 Machine learning examples","heading":"3.2 Random forests","text":"Perhaps concerned true treatment probability expected outcome functions many interactions among predictors. can set treatment_algorithm outcome_algorithm “ranger” estimate via ranger function ranger package (Wright Ziegler 2017).One aspect way gapclosing() operationalizes ranger() unique estimation algorithm options. choose random forest, believe many important interactions. important interactions may treatment predictors. Therefore, outcome_algorithm = ranger enforces interactions estimating outcome model separately treated control units. reason, outcome_algorithm = ranger need include treatment variable explicitly outcome_formula.","code":"\nestimate_ranger <- gapclosing(\n  data = simulated_data,\n  counterfactual_assignments = 1,\n  outcome_formula = formula(outcome ~ confounder + category),\n  treatment_formula = formula(treatment ~ confounder + category),\n  category_name = \"category\",\n  treatment_algorithm = \"ranger\",\n  outcome_algorithm = \"ranger\",\n  sample_split = \"cross_fit\"\n  # Note: Standard errors with `se = TRUE` are supported.\n  # They are omitted here only to speed vignette build time.\n)"},{"path":"machine-learning-examples.html","id":"estimates-from-these-three-algorithms-are-roughly-the-same","chapter":"3 Machine learning examples","heading":"3.3 Estimates from these three algorithms are roughly the same","text":"simulation, GLM models correctly specified nonlinearities interactions machine learning approaches learn. case, sample size large enough approaches correctly learn linear functional form, three estimation strategies yield similar estimates.Note confidence intervals GAM random forest can also generated SE = TRUE, turned speed vignette build time.","code":""},{"path":"machine-learning-examples.html","id":"word-of-warning","chapter":"3 Machine learning examples","heading":"3.4 Word of warning","text":"assumptions parametric model always doubtful, leading common question whether one always use flexible machine learning approach like ranger. large sample, flexible learner likely correct choice. sample sizes social science settings, amount data may sometimes insufficient algorithms discover complex functional form. parametric assumptions approximately true, parametric estimators may better performance small sample sizes. counts “small” “large” difficult say outside specific setting.","code":""},{"path":"advanced-treatment-assignments.html","id":"advanced-treatment-assignments","chapter":"4 Advanced treatment assignments","heading":"4 Advanced treatment assignments","text":"far, focused estimation fixed treatment assignment: assign treatment 1 probability 1.might also want know gap-closing estimandif assigned people treatment stochasticallyif person’s assignment individualized","code":""},{"path":"advanced-treatment-assignments.html","id":"stochastic-treatments","chapter":"4 Advanced treatment assignments","heading":"4.1 Stochastic treatments","text":"may want study counterfactual treatment assigned probability 0 1. counterfactual_assignments argument can handle possibility.example, consider gap-closing estimand assigned treatment 1 probability .75.","code":"\nestimate_stochastic <- gapclosing(\n  data = simulated_data,\n  counterfactual_assignments = .75,\n  outcome_formula = formula(outcome ~ confounder + category*treatment),\n  treatment_formula = formula(treatment ~ confounder + category),\n  category_name = \"category\"\n)"},{"path":"advanced-treatment-assignments.html","id":"individualized-treatments","chapter":"4 Advanced treatment assignments","heading":"4.2 Individualized treatments","text":"Treatment may also different (possibly stochastic) unit counterfactual world interest.example, suppose assign Category treatment 1 probability .5, Category B treatment probability .4, Category C treatment probability .3. case, counterfactual_assignments set vector length nrow(data).intervention close B - gap 31%.","code":"\nour_assignments <- case_when(simulated_data$category == \"A\" ~ .5,\n                             simulated_data$category == \"B\" ~ .4, \n                             simulated_data$category == \"C\" ~ .3)\nestimate_stochastic <- gapclosing(\n  data = simulated_data,\n  counterfactual_assignments = our_assignments,\n  outcome_formula = formula(outcome ~ confounder + category*treatment),\n  treatment_formula = formula(treatment ~ confounder + category),\n  category_name = \"category\"\n)"},{"path":"conclusion.html","id":"conclusion","chapter":"5 Conclusion","heading":"5 Conclusion","text":"gapclosing package designed support inquiry gap closing estimands, thus promoting new understanding interventions can close gaps across social categories. goal package automate technical tasks (sample splitting, aggregation doubly robust estimates, visualization), thus freeing researcher devote attention scientific tasks like defining intervention making causal assumptions.use package find bug, helpful create issue GitHub. Suggestions additional features also welcome.vignette compiled 2023-03-09 08:29:19.","code":""},{"path":"conclusion.html","id":"computing-environment","chapter":"5 Conclusion","heading":"5.1 Computing environment","text":"","code":"\nsessionInfo()\n#> R version 4.2.2 (2022-10-31)\n#> Platform: aarch64-apple-darwin20 (64-bit)\n#> Running under: macOS Ventura 13.2.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n#> \n#> locale:\n#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets \n#> [6] methods   base     \n#> \n#> other attached packages:\n#> [1] ggplot2_3.4.0    dplyr_1.1.0      gapclosing_1.0.2\n#> \n#> loaded via a namespace (and not attached):\n#>  [1] tidyselect_1.2.0  xfun_0.37         bslib_0.4.2      \n#>  [4] purrr_1.0.1       splines_4.2.2     lattice_0.20-45  \n#>  [7] colorspace_2.1-0  vctrs_0.5.2       generics_0.1.3   \n#> [10] htmltools_0.5.4   yaml_2.3.7        mgcv_1.8-41      \n#> [13] utf8_1.2.3        rlang_1.0.6       jquerylib_0.1.4  \n#> [16] pillar_1.8.1      glue_1.6.2        withr_2.5.0      \n#> [19] foreach_1.5.2     lifecycle_1.0.3   munsell_0.5.0    \n#> [22] gtable_0.3.1      codetools_0.2-19  memoise_2.0.1    \n#> [25] evaluate_0.20     labeling_0.4.2    knitr_1.42       \n#> [28] forcats_1.0.0     fastmap_1.1.0     doParallel_1.0.17\n#> [31] parallel_4.2.2    fansi_1.0.4       highr_0.10       \n#> [34] Rcpp_1.0.10       scales_1.2.1      cachem_1.0.6     \n#> [37] jsonlite_1.8.4    farver_2.1.1      fs_1.6.1         \n#> [40] ranger_0.14.1     digest_0.6.31     bookdown_0.33    \n#> [43] grid_4.2.2        cli_3.6.0         tools_4.2.2      \n#> [46] magrittr_2.0.3    sass_0.4.5        tibble_3.1.8     \n#> [49] tidyr_1.3.0       pkgconfig_2.0.3   downlit_0.4.2    \n#> [52] Matrix_1.5-3      xml2_1.3.3        rmarkdown_2.20   \n#> [55] rstudioapi_0.14   iterators_1.0.14  R6_2.5.1         \n#> [58] nlme_3.1-162      compiler_4.2.2"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
